{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning for Atari Games\n",
    "## Learning to play Space Invaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:30.745119Z",
     "start_time": "2020-06-06T15:57:25.042370Z"
    }
   },
   "outputs": [],
   "source": [
    "import random, math\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import atari_wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for plotting average score during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:30.772048Z",
     "start_time": "2020-06-06T15:57:30.754098Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_score(mean_episode_score, episode):\n",
    "        plt.figure()\n",
    "        plt.title('Average Score (over last 100 episodes): ' + str(int(mean_episode_score[-1])) + '  / Episode: ' + str(episode))\n",
    "        plt.xlabel('Episode (x 100)')\n",
    "        plt.ylabel('Average Score (over last 100 episodes)')\n",
    "        plt.plot(mean_episode_score)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        plt.pause(1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we define the device for the PyTorch computation to run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:30.804960Z",
     "start_time": "2020-06-06T15:57:30.779030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(use_cuda)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The agent neural network\n",
    "\n",
    "It is a common practice in PyTorch to define your neural network in a class that extends the **nn.Module** class. In this way, you define the neural network building blocks (usually layers) in the class initialization, and then define only the forward pass operations of your network in the **forward** method.\n",
    "\n",
    "Here we define a Convolutional Neural Network that builds a vector representation of the image information given by the game environment and then uses that vector representation in a feed forward network that computes a Q-value for each possible game action in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:30.840863Z",
     "start_time": "2020-06-06T15:57:30.811944Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQNModel, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "                                  nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "                                  nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.Linear(7 * 7 * 64, 512), nn.ReLU(), nn.Linear(512, n_actions))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs = self.conv(obs)\n",
    "        obs = obs.view(obs.shape[0], obs.shape[1] * obs.shape[2] * obs.shape[3])\n",
    "        actions = self.fc(obs)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the game environment\n",
    "\n",
    "##### We use the environment for the classic Space Invaders game.\n",
    "\n",
    "##### Here we use the Deepmind's wrappers to rescale the image to 84 x 84, single gray scale channel, and stack 4 consecutive frames of the game as one observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:35.054595Z",
     "start_time": "2020-06-06T15:57:30.848843Z"
    }
   },
   "outputs": [],
   "source": [
    "env = atari_wrappers.make_atari('RiverraidNoFrameskip-v4')\n",
    "env = atari_wrappers.wrap_deepmind(env, clip_rewards=False, frame_stack=True, pytorch_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation and action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:35.086511Z",
     "start_time": "2020-06-06T15:57:35.068558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4, 84, 84)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:35.115434Z",
     "start_time": "2020-06-06T15:57:35.096484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space = [a for a in range(env.action_space.n)]\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:35.150342Z",
     "start_time": "2020-06-06T15:57:35.131392Z"
    }
   },
   "outputs": [],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network model objects for the Deep Q-Learning algorithm\n",
    "\n",
    "##### Here we instantiate the model objects. We will be working with two models: one for the **estimate** network and other for the **target** network. The **estimate** network computes Q-values for the executed actions and the **target** network computes Q-values for the expected Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:35.936239Z",
     "start_time": "2020-06-06T15:57:35.797610Z"
    }
   },
   "outputs": [],
   "source": [
    "n_actions = len(action_space)\n",
    "\n",
    "lr = 0.00040\n",
    "alpha = 0.95\n",
    "\n",
    "policy_model = DQNModel().to(device)\n",
    "target_model = DQNModel().to(device)\n",
    "target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "optimizer = torch.optim.RMSprop(policy_model.parameters(), lr=lr, alpha=alpha)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(policy_model)\n",
    "print(\"Number of learnable parameters: %d\" % count_parameters(policy_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent parameters\n",
    "\n",
    "##### Here we define the macro parameters used by the agent when interacting with the environment in the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:37.653645Z",
     "start_time": "2020-06-06T15:57:37.645676Z"
    }
   },
   "outputs": [],
   "source": [
    "max_episodes = 20000\n",
    "batch_size = 48\n",
    "target_update = 500\n",
    "gamma = 0.99\n",
    "rep_buf_size = 50000\n",
    "rep_buf_ini = 5000\n",
    "skip_frame = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-Greedy exploration\n",
    "\n",
    "##### We use an annealing scheme for decreasing the **epsilon** parameter, which controls the exploration vs exploitation tradeoff during training. The idea is to allow the agent to explore a lot in the beginning and then making it to choose more “ informed” actions as the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:41.911260Z",
     "start_time": "2020-06-06T15:57:39.408952Z"
    }
   },
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.05\n",
    "epsilon_decay = 25000\n",
    "\n",
    "epsilon_by_frame = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * step_idx / epsilon_decay)\n",
    "\n",
    "plt.plot([epsilon_by_frame(i) for i in range(500000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Experience Replay buffer\n",
    "\n",
    "##### Here we define the experience replay buffer, which allows the agent to store tuples of (state, action, reward, next state, done), which are the experiences it collects when interacting with the game environment. This buffer has a fixed capacity (in number of tuples), so that when maximum capacity is reached the oldest tuple is removed so that a new one can be inserted.\n",
    "\n",
    "##### This buffer also has a method for the agent to sample experiences at random in order to construct mini-batches of experiences used to train the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:43.942828Z",
     "start_time": "2020-06-06T15:57:43.922887Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huber Loss\n",
    "\n",
    "##### Here we implement the Huber loss function, which makes gradient updates smoother because it makes the network less sensitive to large errors than the loss function based on the Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:57:45.635304Z",
     "start_time": "2020-06-06T15:57:45.624331Z"
    }
   },
   "outputs": [],
   "source": [
    "def huber_loss(input, target, beta=1, size_average=True):\n",
    "    \"\"\"\n",
    "    very similar to the smooth_l1_loss from pytorch, but with\n",
    "    the extra beta parameter\n",
    "    \"\"\"\n",
    "    n = torch.abs(input - target)\n",
    "    cond = n < beta\n",
    "    loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n",
    "    if size_average:\n",
    "        return loss.mean()\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay buffer Initialization\n",
    "\n",
    "##### We initialize the Experience Replay buffer to its entire capacity with random playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:59:21.955250Z",
     "start_time": "2020-06-06T15:57:47.317803Z"
    }
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(rep_buf_size)\n",
    "\n",
    "while len(replay_buffer) < rep_buf_ini:\n",
    "    \n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_observation = torch.from_numpy(observation).float().to(device)\n",
    "            t_observation = t_observation.view(1, t_observation.shape[0], t_observation.shape[1], t_observation.shape[2])\n",
    "            action = random.sample(range(len(action_space)), 1)[0]\n",
    "        \n",
    "        next_observation, reward, done, info = env.step(action_space[action])\n",
    "            \n",
    "        replay_buffer.push(observation, action, reward, next_observation, done)\n",
    "        observation = next_observation\n",
    "        \n",
    "print('Experience Replay buffer initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:59:27.130411Z",
     "start_time": "2020-06-06T15:59:27.118443Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('dqn_spaceinvaders')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger_handler = logging.FileHandler('./data/dqn_spaceinvaders.log')\n",
    "logger.addHandler(logger_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent training loop\n",
    "\n",
    "##### Here we train the agent for a fixed number of episodes. For each episode, the agent keeps track of the current observation, selects and execute an action and collect feedback from the game environment which are the next observation, the immediate reward received, and a flag indicating if the game is done or not and inserts this experience in the buffer.\n",
    "\n",
    "##### For our Space Invaders game in the Atari environment, we consider one episode complete after the agent loses one of its 3 available lives.\n",
    "\n",
    "##### After each **skip_frame** number of game frames played, we update the agent policy, which is the Deep Q-Network that computes the Q-Value estimates, by sampling a mini-batch of experiences from the buffer and updating the network parameters. We also update the target network after a **target_update** fixed number of game frames.\n",
    "\n",
    "##### Finally, we plot and log the average score in the game so far, across the latest 100 game episodes. We also save these scores to be plotted later. And we save the model dictionary to disk each 1000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T15:59:29.466165Z",
     "start_time": "2020-06-06T15:59:29.453201Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "i =1 \n",
    "end=time.time()\n",
    "print(\"Running time ( %i episode): %.3f Seconds \"%(i,end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-06T15:59:32.216Z"
    }
   },
   "outputs": [],
   "source": [
    "episode_score = []\n",
    "mean_episode_score = []\n",
    "\n",
    "num_frames = 0\n",
    "episode = 0\n",
    "score = 0\n",
    "\n",
    "while episode < max_episodes:\n",
    "    \n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    #import time\n",
    "    #start=time.time()\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            t_observation = torch.from_numpy(observation).float().to(device) / 255\n",
    "            t_observation = t_observation.view(1, t_observation.shape[0], t_observation.shape[1], t_observation.shape[2])\n",
    "            epsilon = epsilon_by_frame(num_frames)\n",
    "            if random.random() > epsilon:\n",
    "                q_value = policy_model(t_observation)\n",
    "                action = q_value.argmax(1).data.cpu().numpy().astype(int)[0]\n",
    "            else:\n",
    "                action = random.sample(range(len(action_space)), 1)[0]\n",
    "        \n",
    "        next_observation, reward, done, info = env.step(action_space[action])\n",
    "        num_frames += 1\n",
    "        score += reward\n",
    "            \n",
    "        replay_buffer.push(observation, action, reward, next_observation, done)\n",
    "        observation = next_observation\n",
    "        \n",
    "        # Update policy\n",
    "        if len(replay_buffer) > batch_size and num_frames % skip_frame == 0:\n",
    "            observations, actions, rewards, next_observations, dones = replay_buffer.sample(batch_size)          \n",
    "\n",
    "            observations = torch.from_numpy(np.array(observations) / 255).float().to(device)\n",
    "            \n",
    "            actions = torch.from_numpy(np.array(actions).astype(int)).float().to(device)\n",
    "            actions = actions.view(actions.shape[0], 1)\n",
    "            \n",
    "            rewards = torch.from_numpy(np.array(rewards)).float().to(device)\n",
    "            rewards = rewards.view(rewards.shape[0], 1)\n",
    "            \n",
    "            next_observations = torch.from_numpy(np.array(next_observations) / 255).float().to(device)\n",
    "            \n",
    "            dones = torch.from_numpy(np.array(dones).astype(int)).float().to(device)\n",
    "            dones = dones.view(dones.shape[0], 1)\n",
    "            \n",
    "            q_values = policy_model(observations)\n",
    "            next_q_values = target_model(next_observations)\n",
    "\n",
    "            q_value = q_values.gather(1, actions.long())\n",
    "            next_q_value = next_q_values.max(1)[0].unsqueeze(1)\n",
    "            expected_q_value = rewards + gamma * next_q_value * (1 - dones)\n",
    "\n",
    "            loss = huber_loss(q_value, expected_q_value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        if num_frames % target_update == 0:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "    \n",
    "    episode += 1\n",
    "    episode_score.append(score)\n",
    "    #end=time.time()\n",
    "    #print(\"Running time ( %i episode): %.3f Seconds \"%(episode ,end-start))\n",
    "    \n",
    "    if info['ale.lives'] == 0:\n",
    "        score = 0\n",
    "    \n",
    "    if episode % 2 == 0:\n",
    "        mean_score = np.mean(episode_score)\n",
    "        mean_episode_score.append(mean_score)\n",
    "        episode_score = []\n",
    "        logger.info('Frame: ' + str(num_frames) + ' / Episode: ' + str(episode) + ' / Average Score (over last 2 episodes): ' + str(int(mean_score)))\n",
    "        plot_score(mean_episode_score, episode)\n",
    "        pickle.dump(mean_episode_score, open('./data/dqn_spaceinvaders_mean_scores.pickle', 'wb'))\n",
    "        \n",
    "    if episode % 10 == 0:\n",
    "        torch.save(policy_model.state_dict(), './data/dqn_spaceinvaders_model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and plotting the curve of the average score each 100 episodes for a trained policy over 12,000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_episode_score = pickle.load(open('./data/dqn_spaceinvaders_mean_scores.pickle', \"rb\"))\n",
    "\n",
    "plot_score(mean_episode_score, 12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the network parameters of the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = DQNModel().to(device)\n",
    "policy_model.load_state_dict(torch.load('./data/dqn_spaceinvaders_model_state_dict.pt'))\n",
    "policy_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = atari_wrappers.make_atari('SpaceInvadersNoFrameskip-v4')\n",
    "env = atari_wrappers.wrap_deepmind(env, clip_rewards=True, frame_stack=True, pytorch_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing the game with a trained agent\n",
    "\n",
    "##### Here we test a trained agent in a loop like the training loop defined before, except that here we choose the action in a greedily manner and don’t store experiences in the buffer neither update the policy parameters.\n",
    "\n",
    "##### We run the loop for 3 episodes, corresponding to the 3 available lives in one game play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3\n",
    "episode = 1\n",
    "\n",
    "plt.figure(figsize = (6,9))\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "while episode <= num_episodes:\n",
    "    \n",
    "    observation = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    episode += 1\n",
    "    \n",
    "    while not done:\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            t_observation = torch.from_numpy(observation).float().to(device) / 255\n",
    "            t_observation = t_observation.view(1, t_observation.shape[0], t_observation.shape[1], t_observation.shape[2])\n",
    "            q_value = policy_model(t_observation)\n",
    "            action = q_value.argmax(1).data.cpu().numpy().astype(int)[0]\n",
    "        \n",
    "        next_observation, reward, done, info = env.step(action_space[action])\n",
    "        \n",
    "        observation = next_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing the game with a random agent\n",
    "\n",
    "##### We define here a random agent just as a baseline to compare to trained agents. As the name suggests, this agent executes random actions in the environment. Those actions are chosen randomly from the set of actions available to the game environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "episode = 1\n",
    "\n",
    "plt.figure(figsize = (6,9))\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "while episode <= num_episodes:\n",
    "    \n",
    "    observation = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    episode += 1\n",
    "    \n",
    "    while not done:\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        action = random.sample(range(len(action_space)), 1)[0]\n",
    "        \n",
    "        next_observation, reward, done, info = env.step(action_space[action])\n",
    "        \n",
    "        observation = next_observation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (untitled)",
   "language": "python",
   "name": "pycharm-deebe7a9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
